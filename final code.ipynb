{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo3fp2xl1CKg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch nltk datasets Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel,ViTFeatureExtractor, ViTImageProcessor, AutoTokenizer,  BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from PIL import Image\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "# Constants\n",
        "max_length = 15\n",
        "num_beams = 6\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"biglam/dating-historical-color-images\")\n",
        "\n",
        "decade_labels = {\n",
        "    0: '1930',\n",
        "    1: '1940',\n",
        "    2: '1950',\n",
        "    3: '1960',\n",
        "    4: '1970'\n",
        "}\n",
        "\n",
        "\n",
        "class HistoricalImageDataset(Dataset):\n",
        "    def __init__(self, dataset, captions= None, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.captions = captions\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image'].convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Append the date to the pre-generated caption\n",
        "        date = decade_labels[item['label']]\n",
        "        full_caption = self.captions[idx].rstrip() + f' during {date}'\n",
        "\n",
        "        return image, full_caption\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Transformations for the image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = HistoricalImageDataset(dataset=dataset['train'], transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "data_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mTrFngeI1ORp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(), lr=4e-5)\n",
        "num_epochs = 5\n",
        "criterion = CrossEntropyLoss()\n",
        "print_interval = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, captions) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        inputs = tokenizer(captions, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        input_ids = inputs.input_ids.to(device)\n",
        "\n",
        "        # Prepare shifted labels for the decoder\n",
        "        labels = torch.roll(input_ids, -1, dims=1)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "        # Check and compute loss\n",
        "        loss = outputs.loss if outputs.loss is not None else criterion(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if (i + 1) % print_interval == 0:\n",
        "            # Print caption every 'print_interval' iterations\n",
        "            print(\"Caption:\", captions)\n",
        "            print(f\"Iteration [{i + 1}/{len(data_loader)}], Epoch {epoch + 1}/{num_epochs}, Total Loss: {total_loss}\")\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Total Loss: {total_loss}\")\n"
      ],
      "metadata": {
        "id": "k2KDN3PYExIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0359fc3-ba9f-437a-96e0-827d1d840690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption: ('a mountain range with a river and mountains during 1950', 'a large building with a clock on top of it during 1940')\n",
            "Iteration [100/663], Epoch 1/5, Total Loss: 219.33620190620422\n",
            "Caption: ('a man standing on top of a hill holding a stick during 1930', 'a double decker bus parked in front of a brick building during 1970')\n",
            "Iteration [200/663], Epoch 1/5, Total Loss: 387.8956495523453\n",
            "Caption: ('a beach filled with lots of sand and water during 1970', 'a large group of people standing in front of a sign during 1960')\n",
            "Iteration [300/663], Epoch 1/5, Total Loss: 542.3875263929367\n",
            "Caption: ('two little girls standing next to each other during 1950', 'a man standing in front of a truck during 1940')\n",
            "Iteration [400/663], Epoch 1/5, Total Loss: 692.8588055968285\n",
            "Caption: ('a train on a train track during 1930', 'a large building with a clock on the front of it during 1950')\n",
            "Iteration [500/663], Epoch 1/5, Total Loss: 836.9095251560211\n",
            "Caption: ('a man standing next to an old fashioned oven during 1930', 'a statue of a man riding a horse in front of a fountain during 1960')\n",
            "Iteration [600/663], Epoch 1/5, Total Loss: 986.6614870429039\n",
            "Epoch 1/5, Total Loss: 1078.5742672681808\n",
            "Caption: ('a city with lots of tall buildings and trees during 1970', 'men standing next to each other during 1930')\n",
            "Iteration [100/663], Epoch 2/5, Total Loss: 127.3917230963707\n",
            "Caption: ('a train that is sitting on the tracks during 1970', 'a woman sitting on the floor playing a video game during 1950')\n",
            "Iteration [200/663], Epoch 2/5, Total Loss: 254.66776883602142\n",
            "Caption: ('a large stone wall with a mountain range behind it during 1940', 'a bridge over a body of water with a view of a city during 1940')\n",
            "Iteration [300/663], Epoch 2/5, Total Loss: 382.39093577861786\n",
            "Caption: ('two women standing next to each other in front of a building during 1960', 'a fire hydrant on the side of a dirt road during 1960')\n",
            "Iteration [400/663], Epoch 2/5, Total Loss: 512.6054328083992\n",
            "Caption: ('a woman sitting on the ground with a hat on during 1940', 'a man in a cowboy hat is holding a cow during 1930')\n",
            "Iteration [500/663], Epoch 2/5, Total Loss: 647.1990596652031\n",
            "Caption: ('a large clock tower towering over a city during 1940', 'a man standing in front of a fire hydrant during 1950')\n",
            "Iteration [600/663], Epoch 2/5, Total Loss: 777.2793502211571\n",
            "Epoch 2/5, Total Loss: 856.7679740786552\n",
            "Caption: ('a house that has a tree in front of it during 1940', 'a small plane is parked in the middle of an airport during 1930')\n",
            "Iteration [100/663], Epoch 3/5, Total Loss: 115.5358836054802\n",
            "Caption: (\"a large building with a large clock on it's side during 1940\", 'a red and white fire truck driving down a street during 1950')\n",
            "Iteration [200/663], Epoch 3/5, Total Loss: 234.5202795267105\n",
            "Caption: ('a large group of people sitting on top of a grass covered field during 1970', 'a train on a track near a train station during 1970')\n",
            "Iteration [300/663], Epoch 3/5, Total Loss: 352.8823861479759\n",
            "Caption: ('a pool of water filled with snow next to a fence during 1950', 'a crowd of people walking down a city street during 1950')\n",
            "Iteration [400/663], Epoch 3/5, Total Loss: 472.42099326848984\n",
            "Caption: ('a man and a woman sitting on a park bench during 1950', 'a little boy sitting on the ground next to a dog during 1940')\n",
            "Iteration [500/663], Epoch 3/5, Total Loss: 591.6230082511902\n",
            "Caption: ('a brick building with a brick wall during 1970', 'a large truck on a dock near a body of water during 1960')\n",
            "Iteration [600/663], Epoch 3/5, Total Loss: 714.5660009384155\n",
            "Epoch 3/5, Total Loss: 788.6603637933731\n",
            "Caption: ('a motorcycle that is sitting on top of a table during 1960', 'a man standing in front of a sign during 1950')\n",
            "Iteration [100/663], Epoch 4/5, Total Loss: 106.52620381116867\n",
            "Caption: ('a display case filled with lots of apples during 1930', 'two men sitting on a bench next to each other during 1960')\n",
            "Iteration [200/663], Epoch 4/5, Total Loss: 215.29447382688522\n",
            "Caption: ('a woman sitting on a bench with a baby during 1940', 'a yellow and blue train traveling down train tracks during 1970')\n",
            "Iteration [300/663], Epoch 4/5, Total Loss: 328.13079714775085\n",
            "Caption: ('a fence is in the middle of a grassy field during 1950', 'a red and black motorcycle is driving down the road during 1960')\n",
            "Iteration [400/663], Epoch 4/5, Total Loss: 442.1685993671417\n",
            "Caption: ('a person holding a fish in their hand during 1970', 'people standing in front of a large building during 1940')\n",
            "Iteration [500/663], Epoch 4/5, Total Loss: 553.3619277477264\n",
            "Caption: ('a white and black fire hydrant sitting on top of a field during 1960', 'a large white boat sitting on top of a body of water during 1940')\n",
            "Iteration [600/663], Epoch 4/5, Total Loss: 664.4299419522285\n",
            "Epoch 4/5, Total Loss: 734.2473002672195\n",
            "Caption: ('a red and white fire hydrant next to a fence during 1970', 'a woman and a child riding a bike down a street during 1950')\n",
            "Iteration [100/663], Epoch 5/5, Total Loss: 99.52793174982071\n",
            "Caption: ('a crowd of people standing in front of a car during 1950', 'a flock of birds flying over a body of water during 1960')\n",
            "Iteration [200/663], Epoch 5/5, Total Loss: 206.79234570264816\n",
            "Caption: ('a small boat sitting on top of a dirt field during 1930', 'a man standing next to a propeller on a motorcycle during 1930')\n",
            "Iteration [300/663], Epoch 5/5, Total Loss: 314.5198367834091\n",
            "Caption: ('people are gathered around a table with food during 1940', 'a man holding a bird on top of a skateboard during 1930')\n",
            "Iteration [400/663], Epoch 5/5, Total Loss: 420.7442143559456\n",
            "Caption: ('a large group of people swimming in the water during 1940', 'a large body of water with a bridge over it during 1950')\n",
            "Iteration [500/663], Epoch 5/5, Total Loss: 525.2747268080711\n",
            "Caption: ('a little girl sitting in the grass with a frisbee during 1950', 'a woman sitting on a couch holding a teddy bear during 1950')\n",
            "Iteration [600/663], Epoch 5/5, Total Loss: 631.3839777708054\n",
            "Epoch 5/5, Total Loss: 697.919022321701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def generate_caption_for_url_image(model, feature_extractor, url, max_length=16, num_beams=4):\n",
        "    \"\"\"\n",
        "    Generate a caption for an image obtained from a URL.\n",
        "\n",
        "    Parameters:\n",
        "    model: The loaded VisionEncoderDecoderModel.\n",
        "    feature_extractor: The loaded feature extractor for the model.\n",
        "    url (str): URL of the image.\n",
        "    max_length (int): Maximum length of the generated caption.\n",
        "    num_beams (int): Number of beams for beam search.\n",
        "\n",
        "    Returns:\n",
        "    str: Generated caption for the image.\n",
        "    \"\"\"\n",
        "    # Download the image from the URL\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # Process the image\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(model.device)\n",
        "\n",
        "    # Generate caption\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(pixel_values, max_length=max_length, num_beams=num_beams)\n",
        "\n",
        "    # Decode the generated caption\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcShbm3S6hBp1pfAM43tpHpdyf_8Mgvx1-MzQlIGp2xFFQ&s'\n",
        "generated_caption = generate_caption_for_url_image(model, feature_extractor, url)\n",
        "real_caption = 'a car parked in front of a building during 1950'\n",
        "print(generated_caption)\n",
        "real_tokens = nltk.word_tokenize(real_caption.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_caption.lower())\n",
        "\n",
        "\n",
        "real_caption_list = [real_tokens]\n",
        "generated_caption_list = [generated_tokens]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu([real_caption], [generated_caption], weights = (1,0,0,0))\n",
        "print(f\"BLEU score: {bleu_score}\")\n",
        "\n",
        "\n",
        "file_path = \"saved_images.txt\"\n",
        "\n",
        "with open(file_path, \"w\") as file:\n",
        "    for real_sent, generated_sent in zip(real_caption_list, generated_caption_list):\n",
        "        real_sent_str = \" \".join(real_sent)\n",
        "        generated_sent_str = \" \".join(generated_sent)\n",
        "\n",
        "        file.write(f\"Real Caption: {real_sent_str}\\n\")\n",
        "        file.write(f\"Generated Caption: {generated_sent_str}\\n\")\n",
        "\n",
        "print(f\"Captions saved to {file_path}\")\n"
      ],
      "metadata": {
        "id": "y2Zwy2Zh9Un7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "def numbers_match(string1, string2):\n",
        "    \"\"\"\n",
        "    Check if any numbers present in string1 are also present in string2.\n",
        "\n",
        "    Args:\n",
        "    string1 (str): The first string to be analyzed.\n",
        "    string2 (str): The second string to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if any number in string1 is found in string2, False otherwise.\n",
        "    \"\"\"\n",
        "    # Extracting numbers from the first string\n",
        "    numbers_in_string1 = set(filter(str.isdigit, string1))\n",
        "\n",
        "    # Checking if any number from string1 exists in string2\n",
        "    for number in numbers_in_string1:\n",
        "        if number in string2:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "import os\n",
        "\n",
        "directory_path = \"saved_images\"\n",
        "\n",
        "count = 0\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        real_caption = file.readline().strip()\n",
        "        generated_caption = file.readline().strip()\n",
        "\n",
        "    if numbers_match(real_caption, generated_caption):\n",
        "        print(\"Success!\")\n",
        "        count += 1\n",
        "    else:\n",
        "        print(\"No Match!\")\n",
        "        count += 1\n",
        "\n",
        "print(f\"Total matches found: {count}\")"
      ],
      "metadata": {
        "id": "cQvnrZGIjbyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F1 Score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_matrix = confusion_matrix(real_caption_list, generated_caption_list)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "precision = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n",
        "recall = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Print F1 scores for each class\n",
        "for i, date_range in enumerate([\"1930s\", \"1940s\", \"1950s\", \"1960s\", \"1970s\"]):\n",
        "    print(f\"F1 Score ({date_range}): {f1_score[i]:.2f}\")\n",
        "\n",
        "# Calculate weighted average F1 score\n",
        "weights = np.sum(confusion_matrix, axis=1) / np.sum(confusion_matrix)\n",
        "weighted_average_f1_score = np.sum(f1_score * weights)\n",
        "\n",
        "print(f\"Weighted Average F1 Score: {weighted_average_f1_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "sB3_kAOKGVE7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
