{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo3fp2xl1CKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf001202-5f4c-4771-b190-ec1aa2b5e820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch nltk datasets Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel,ViTFeatureExtractor, ViTImageProcessor, AutoTokenizer,  BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from PIL import Image\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "# Constants\n",
        "max_length = 15\n",
        "num_beams = 6\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"biglam/dating-historical-color-images\")\n",
        "\n",
        "decade_labels = {\n",
        "    0: '1930',\n",
        "    1: '1940',\n",
        "    2: '1950',\n",
        "    3: '1960',\n",
        "    4: '1970'\n",
        "}\n",
        "\n",
        "\n",
        "class HistoricalImageDataset(Dataset):\n",
        "    def __init__(self, dataset, captions, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.captions = captions\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image'].convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Append the date to the pre-generated caption\n",
        "        date = decade_labels[item['label']]\n",
        "        full_caption = self.captions[idx].rstrip() + f' during {date}'\n",
        "\n",
        "        return image, full_caption\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Transformations for the image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = HistoricalImageDataset(dataset=dataset['train'], captions=captions, transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "data_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mTrFngeI1ORp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(), lr=4e-5)\n",
        "num_epochs = 5\n",
        "criterion = CrossEntropyLoss()\n",
        "print_interval = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, captions) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        inputs = tokenizer(captions, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        input_ids = inputs.input_ids.to(device)\n",
        "\n",
        "        # Prepare shifted labels for the decoder\n",
        "        labels = torch.roll(input_ids, -1, dims=1)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "        # Check and compute loss\n",
        "        loss = outputs.loss if outputs.loss is not None else criterion(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if (i + 1) % print_interval == 0:\n",
        "            # Print caption every 'print_interval' iterations\n",
        "            print(\"Caption:\", captions)\n",
        "            print(f\"Iteration [{i + 1}/{len(data_loader)}], Epoch {epoch + 1}/{num_epochs}, Total Loss: {total_loss}\")\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Total Loss: {total_loss}\")\n"
      ],
      "metadata": {
        "id": "k2KDN3PYExIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0359fc3-ba9f-437a-96e0-827d1d840690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption: ('a mountain range with a river and mountains during 1950', 'a large building with a clock on top of it during 1940')\n",
            "Iteration [100/663], Epoch 1/5, Total Loss: 219.33620190620422\n",
            "Caption: ('a man standing on top of a hill holding a stick during 1930', 'a double decker bus parked in front of a brick building during 1970')\n",
            "Iteration [200/663], Epoch 1/5, Total Loss: 387.8956495523453\n",
            "Caption: ('a beach filled with lots of sand and water during 1970', 'a large group of people standing in front of a sign during 1960')\n",
            "Iteration [300/663], Epoch 1/5, Total Loss: 542.3875263929367\n",
            "Caption: ('two little girls standing next to each other during 1950', 'a man standing in front of a truck during 1940')\n",
            "Iteration [400/663], Epoch 1/5, Total Loss: 692.8588055968285\n",
            "Caption: ('a train on a train track during 1930', 'a large building with a clock on the front of it during 1950')\n",
            "Iteration [500/663], Epoch 1/5, Total Loss: 836.9095251560211\n",
            "Caption: ('a man standing next to an old fashioned oven during 1930', 'a statue of a man riding a horse in front of a fountain during 1960')\n",
            "Iteration [600/663], Epoch 1/5, Total Loss: 986.6614870429039\n",
            "Epoch 1/5, Total Loss: 1078.5742672681808\n",
            "Caption: ('a city with lots of tall buildings and trees during 1970', 'men standing next to each other during 1930')\n",
            "Iteration [100/663], Epoch 2/5, Total Loss: 127.3917230963707\n",
            "Caption: ('a train that is sitting on the tracks during 1970', 'a woman sitting on the floor playing a video game during 1950')\n",
            "Iteration [200/663], Epoch 2/5, Total Loss: 254.66776883602142\n",
            "Caption: ('a large stone wall with a mountain range behind it during 1940', 'a bridge over a body of water with a view of a city during 1940')\n",
            "Iteration [300/663], Epoch 2/5, Total Loss: 382.39093577861786\n",
            "Caption: ('two women standing next to each other in front of a building during 1960', 'a fire hydrant on the side of a dirt road during 1960')\n",
            "Iteration [400/663], Epoch 2/5, Total Loss: 512.6054328083992\n",
            "Caption: ('a woman sitting on the ground with a hat on during 1940', 'a man in a cowboy hat is holding a cow during 1930')\n",
            "Iteration [500/663], Epoch 2/5, Total Loss: 647.1990596652031\n",
            "Caption: ('a large clock tower towering over a city during 1940', 'a man standing in front of a fire hydrant during 1950')\n",
            "Iteration [600/663], Epoch 2/5, Total Loss: 777.2793502211571\n",
            "Epoch 2/5, Total Loss: 856.7679740786552\n",
            "Caption: ('a house that has a tree in front of it during 1940', 'a small plane is parked in the middle of an airport during 1930')\n",
            "Iteration [100/663], Epoch 3/5, Total Loss: 115.5358836054802\n",
            "Caption: (\"a large building with a large clock on it's side during 1940\", 'a red and white fire truck driving down a street during 1950')\n",
            "Iteration [200/663], Epoch 3/5, Total Loss: 234.5202795267105\n",
            "Caption: ('a large group of people sitting on top of a grass covered field during 1970', 'a train on a track near a train station during 1970')\n",
            "Iteration [300/663], Epoch 3/5, Total Loss: 352.8823861479759\n",
            "Caption: ('a pool of water filled with snow next to a fence during 1950', 'a crowd of people walking down a city street during 1950')\n",
            "Iteration [400/663], Epoch 3/5, Total Loss: 472.42099326848984\n",
            "Caption: ('a man and a woman sitting on a park bench during 1950', 'a little boy sitting on the ground next to a dog during 1940')\n",
            "Iteration [500/663], Epoch 3/5, Total Loss: 591.6230082511902\n",
            "Caption: ('a brick building with a brick wall during 1970', 'a large truck on a dock near a body of water during 1960')\n",
            "Iteration [600/663], Epoch 3/5, Total Loss: 714.5660009384155\n",
            "Epoch 3/5, Total Loss: 788.6603637933731\n",
            "Caption: ('a motorcycle that is sitting on top of a table during 1960', 'a man standing in front of a sign during 1950')\n",
            "Iteration [100/663], Epoch 4/5, Total Loss: 106.52620381116867\n",
            "Caption: ('a display case filled with lots of apples during 1930', 'two men sitting on a bench next to each other during 1960')\n",
            "Iteration [200/663], Epoch 4/5, Total Loss: 215.29447382688522\n",
            "Caption: ('a woman sitting on a bench with a baby during 1940', 'a yellow and blue train traveling down train tracks during 1970')\n",
            "Iteration [300/663], Epoch 4/5, Total Loss: 328.13079714775085\n",
            "Caption: ('a fence is in the middle of a grassy field during 1950', 'a red and black motorcycle is driving down the road during 1960')\n",
            "Iteration [400/663], Epoch 4/5, Total Loss: 442.1685993671417\n",
            "Caption: ('a person holding a fish in their hand during 1970', 'people standing in front of a large building during 1940')\n",
            "Iteration [500/663], Epoch 4/5, Total Loss: 553.3619277477264\n",
            "Caption: ('a white and black fire hydrant sitting on top of a field during 1960', 'a large white boat sitting on top of a body of water during 1940')\n",
            "Iteration [600/663], Epoch 4/5, Total Loss: 664.4299419522285\n",
            "Epoch 4/5, Total Loss: 734.2473002672195\n",
            "Caption: ('a red and white fire hydrant next to a fence during 1970', 'a woman and a child riding a bike down a street during 1950')\n",
            "Iteration [100/663], Epoch 5/5, Total Loss: 99.52793174982071\n",
            "Caption: ('a crowd of people standing in front of a car during 1950', 'a flock of birds flying over a body of water during 1960')\n",
            "Iteration [200/663], Epoch 5/5, Total Loss: 206.79234570264816\n",
            "Caption: ('a small boat sitting on top of a dirt field during 1930', 'a man standing next to a propeller on a motorcycle during 1930')\n",
            "Iteration [300/663], Epoch 5/5, Total Loss: 314.5198367834091\n",
            "Caption: ('people are gathered around a table with food during 1940', 'a man holding a bird on top of a skateboard during 1930')\n",
            "Iteration [400/663], Epoch 5/5, Total Loss: 420.7442143559456\n",
            "Caption: ('a large group of people swimming in the water during 1940', 'a large body of water with a bridge over it during 1950')\n",
            "Iteration [500/663], Epoch 5/5, Total Loss: 525.2747268080711\n",
            "Caption: ('a little girl sitting in the grass with a frisbee during 1950', 'a woman sitting on a couch holding a teddy bear during 1950')\n",
            "Iteration [600/663], Epoch 5/5, Total Loss: 631.3839777708054\n",
            "Epoch 5/5, Total Loss: 697.919022321701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "print(\"Evaluating metrics...\")\n",
        "\n",
        "\n",
        "def generate_caption_for_url_image(model, feature_extractor, url, max_length=16, num_beams=4):\n",
        "    \"\"\"\n",
        "    Generate a caption for an image obtained from a URL.\n",
        "\n",
        "    Parameters:\n",
        "    model: The loaded VisionEncoderDecoderModel.\n",
        "    feature_extractor: The loaded feature extractor for the model.\n",
        "    url (str): URL of the image.\n",
        "    max_length (int): Maximum length of the generated caption.\n",
        "    num_beams (int): Number of beams for beam search.\n",
        "\n",
        "    Returns:\n",
        "    str: Generated caption for the image.\n",
        "    \"\"\"\n",
        "    # Download the image from the URL\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # Process the image\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(model.device)\n",
        "\n",
        "    # Generate caption\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(pixel_values, max_length=max_length, num_beams=num_beams)\n",
        "\n",
        "    # Decode the generated caption\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "url = 'https://i.insider.com/4dd3dedbcadcbb92571b0000?width=1000&format=jpeg&auto=webp'\n",
        "generated_caption = generate_caption_for_url_image(model, feature_extractor, url)\n",
        "real_caption = 'a group of woman pose for a picture in the 1940s'\n",
        "print(generated_caption)\n",
        "real_tokens = nltk.word_tokenize(real_caption.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_caption.lower())\n",
        "\n",
        "# Convert the individual tokens into lists of sentences\n",
        "real_caption_list = [real_tokens]\n",
        "generated_caption_list = [generated_tokens]\n",
        "\n",
        "\n",
        "file_path = \"saved_images\"\n",
        "\n",
        "with open(file_path, \"w\") as file:\n",
        "    for real_sent, generated_sent in zip(real_caption_list, generated_caption_list):\n",
        "        real_sent_str = \" \".join(real_sent)\n",
        "        generated_sent_str = \" \".join(generated_sent)\n",
        "\n",
        "        file.write(f\"Real Caption: {real_sent_str}\\n\")\n",
        "        file.write(f\"Generated Caption: {generated_sent_str}\\n\")\n",
        "\n",
        "print(f\"Captions saved to {file_path}\")"
      ],
      "metadata": {
        "id": "y2Zwy2Zh9Un7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275df7b7-9265-462d-f584-7c5f2e2e4775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " large group of people standing in front of a building during 1950a\n",
            "BLEU score: 9.788429383461836e-232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "def numbers_match(string1, string2):\n",
        "    \"\"\"\n",
        "    Check if any numbers present in string1 are also present in string2.\n",
        "\n",
        "    Args:\n",
        "    string1 (str): The first string to be analyzed.\n",
        "    string2 (str): The second string to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if any number in string1 is found in string2, False otherwise.\n",
        "    \"\"\"\n",
        "    # Extracting numbers from the first string\n",
        "    numbers_in_string1 = set(filter(str.isdigit, string1))\n",
        "\n",
        "    # Checking if any number from string1 exists in string2\n",
        "    for number in numbers_in_string1:\n",
        "        if number in string2:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "import os\n",
        "\n",
        "directory_path = \"saved_images\"\n",
        "\n",
        "count = 0\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        real_caption = file.readline().strip()\n",
        "        generated_caption = file.readline().strip()\n",
        "\n",
        "    if numbers_match(real_caption, generated_caption):\n",
        "        print(\"Success!\")\n",
        "        count += 1\n",
        "    else:\n",
        "        print(\"No Match!\")\n",
        "        count += 1\n",
        "\n",
        "print(f\"Total matches found: {count}\")"
      ],
      "metadata": {
        "id": "cQvnrZGIjbyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4VC0MTSjbzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def evaluate_caption(generated_caption: str, reference_caption: str) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate a generated caption against a single reference caption using the BLEU score.\n",
        "\n",
        "    Parameters:\n",
        "    generated_caption (str): The caption generated by the model.\n",
        "    reference_caption (str): A single reference caption.\n",
        "\n",
        "    Returns:\n",
        "    float: The BLEU score for the generated caption.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the captions into words\n",
        "    tokenized_generated_caption = generated_caption.split()\n",
        "    tokenized_reference_caption = reference_caption.split()\n",
        "\n",
        "    # Format the generated caption for BLEU evaluation (list of lists)\n",
        "    candidate = [tokenized_generated_caption]\n",
        "\n",
        "    # Format the reference caption for BLEU evaluation (list of list of lists)\n",
        "    references = [[tokenized_reference_caption]]\n",
        "\n",
        "    # Compute the BLEU score\n",
        "    bleu_score = corpus_bleu(references, candidate)\n",
        "    return bleu_score\n",
        "score = evaluate_caption(generated_caption, real_caption)\n",
        "print(f\"BLEU score: {score}\")\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in data_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "CN6jnqI64y-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "6284d590-39bb-4831-ca72-323c91345f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating metrics...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-756eae977762>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating metrics...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_captions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muntrained_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_captions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F! Score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_matrix = confusion_matrix(real_caption, generated_captino)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "precision = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n",
        "recall = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Print F1 scores for each class\n",
        "for i, date_range in enumerate([\"1930s\", \"1940s\", \"1950s\", \"1960s\", \"1970s\"]):\n",
        "    print(f\"F1 Score ({date_range}): {f1_score[i]:.2f}\")\n",
        "\n",
        "# Calculate weighted average F1 score\n",
        "weights = np.sum(confusion_matrix, axis=1) / np.sum(confusion_matrix)\n",
        "weighted_average_f1_score = np.sum(f1_score * weights)\n",
        "\n",
        "print(f\"Weighted Average F1 Score: {weighted_average_f1_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "sB3_kAOKGVE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11MqXWpOHyze"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}